{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec analyses.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eNm682s_Ekw4","colab_type":"code","outputId":"22b75667-ce88-46df-add6-16e581c0b1d8","executionInfo":{"status":"ok","timestamp":1585370068850,"user_tz":420,"elapsed":395,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ufx_JEN9E04v","colab_type":"code","colab":{}},"source":["from gensim.models import phrases, word2vec, KeyedVectors\n","import nltk\n","import numpy as np\n","import codecs\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import argparse\n","import os\n","import string\n","import re\n","from scipy.stats import ttest_ind"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp1xcn6pFsEO","colab_type":"code","outputId":"ce3cf580-02d0-488a-8288-55bcbbd671de","executionInfo":{"status":"ok","timestamp":1585370072374,"user_tz":420,"elapsed":398,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["nltk.download(\"stopwords\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"9l4KRP7iF5Rb","colab_type":"code","colab":{}},"source":["root_dir = \"/content/drive/My Drive/Polygence/Audrey/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhkOL_-uFAEl","colab_type":"text"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"r2EmzhpaE3X6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":649},"outputId":"8a48706f-62c2-46d0-9279-3c696d60d366","executionInfo":{"status":"ok","timestamp":1585370076422,"user_tz":420,"elapsed":2339,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}}},"source":["# load data\n","import pandas as pd\n","data = pd.read_csv(root_dir + \"/data/all_articles_dependencies.csv\")\n","data.head()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>source</th>\n","      <th>author</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>publishedAt</th>\n","      <th>content</th>\n","      <th>coref_resolved</th>\n","      <th>Bennet_verbs</th>\n","      <th>Bennet_adjectives</th>\n","      <th>Biden_verbs</th>\n","      <th>Biden_adjectives</th>\n","      <th>Blasio_verbs</th>\n","      <th>Blasio_adjectives</th>\n","      <th>Booker_verbs</th>\n","      <th>Booker_adjectives</th>\n","      <th>Bullock_verbs</th>\n","      <th>Bullock_adjectives</th>\n","      <th>Buttigieg_verbs</th>\n","      <th>Buttigieg_adjectives</th>\n","      <th>Castro_verbs</th>\n","      <th>Castro_adjectives</th>\n","      <th>Delaney_verbs</th>\n","      <th>Delaney_adjectives</th>\n","      <th>Gabbard_verbs</th>\n","      <th>Gabbard_adjectives</th>\n","      <th>Gillibrand_verbs</th>\n","      <th>Gillibrand_adjectives</th>\n","      <th>Harris_verbs</th>\n","      <th>Harris_adjectives</th>\n","      <th>Inslee_verbs</th>\n","      <th>Inslee_adjectives</th>\n","      <th>Klobuchar_verbs</th>\n","      <th>Klobuchar_adjectives</th>\n","      <th>Moulton_verbs</th>\n","      <th>Moulton_adjectives</th>\n","      <th>O'Rourke_verbs</th>\n","      <th>O'Rourke_adjectives</th>\n","      <th>Ryan_verbs</th>\n","      <th>Ryan_adjectives</th>\n","      <th>Sanders_verbs</th>\n","      <th>Sanders_adjectives</th>\n","      <th>Steyer_verbs</th>\n","      <th>Steyer_adjectives</th>\n","      <th>Warren_verbs</th>\n","      <th>Warren_adjectives</th>\n","      <th>Williamson_verbs</th>\n","      <th>Williamson_adjectives</th>\n","      <th>Yang_verbs</th>\n","      <th>Yang_adjectives</th>\n","      <th>Bloomberg_verbs</th>\n","      <th>Bloomberg_adjectives</th>\n","      <th>month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>abc-news</td>\n","      <td>John Verhovek</td>\n","      <td>5 takeaways from the 2nd night of the Democrat...</td>\n","      <td>https://abcnews.go.com/Politics/takeaways-2nd-...</td>\n","      <td>2019-08-01T03:31:25Z</td>\n","      <td>\\nEven before all the candidates took the stag...</td>\n","      <td>\\nEven before all the candidates took the stag...</td>\n","      <td>had 1\\nsaid 1</td>\n","      <td>NaN</td>\n","      <td>said 3\\nsparred 1\\ncriticized 1\\nhit 1\\nlearne...</td>\n","      <td>NaN</td>\n","      <td>warned 1\\nbelieve 1</td>\n","      <td>NaN</td>\n","      <td>said 2\\ncriticized 1\\nimplored 1\\nwas 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 2\\nwent 1\\nargued 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 1</td>\n","      <td>NaN</td>\n","      <td>challenged 2\\ntook 1\\nsaid 1\\nmade 1</td>\n","      <td>concerned 1</td>\n","      <td>were 1\\ncriticized 1\\nsaid 1\\noffered 1\\navoid 1</td>\n","      <td>NaN</td>\n","      <td>hit 1\\nimplored 1\\nwas 1\\nsaid 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>was 1\\nsaid 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>aug</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>abc-news</td>\n","      <td>ABC NEWS</td>\n","      <td>Fact-checking Democratic candidates on the iss...</td>\n","      <td>https://abcnews.go.com/Politics/democratic-deb...</td>\n","      <td>2019-08-01T01:29:38Z</td>\n","      <td>\\nHere's ABC News' fact check of the second of...</td>\n","      <td>\\nHere's ABC News' fact check of the second of...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 4\\nwas 2\\nwrote 2\\nresponded 2\\nvoted 2\\n...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>inherited 1\\nput 1\\nam 1\\nhire 1\\nwrote 1\\nstr...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>'m 1</td>\n","      <td>| 1</td>\n","      <td>is 3\\nthink 1\\nbelieve 1\\nraised 1</td>\n","      <td>NaN</td>\n","      <td>said 3\\nkept 2\\nacknowledging 2\\nran 2\\ndefend...</td>\n","      <td>| 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 1\\nbelieve 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>aug</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>abc-news</td>\n","      <td>Kendall Karson and Elizabeth Thomas</td>\n","      <td>What to expect from 2nd night of Democratic de...</td>\n","      <td>https://abcnews.go.com/Politics/democratic-deb...</td>\n","      <td>2019-07-31T20:01:36Z</td>\n","      <td>\\nAfter Sens. Bernie Sanders and Elizabeth War...</td>\n","      <td>\\nAfter Sens. Bernie Sanders and Elizabeth War...</td>\n","      <td>said 5\\nchimed 1\\nresponded 1\\ncontinued 1\\ncr...</td>\n","      <td>NaN</td>\n","      <td>said 11\\nresponded 4\\nwas 3\\nis 2\\nasked 2\\npu...</td>\n","      <td>old 1</td>\n","      <td>said 3\\noppose 1\\nmake 1\\npressed 1\\nasked 1\\n...</td>\n","      <td>NaN</td>\n","      <td>said 5\\nthink 2\\nendorsed 2\\ncriticized 1\\nwan...</td>\n","      <td>glad 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 6\\nbelieve 2\\nwant 2\\npushed 1\\nthink 1\\n...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 4\\nasks 1\\nbelieves 1\\nbacked 1\\nshared 1...</td>\n","      <td>old 1</td>\n","      <td>said 6\\nasked 2\\nfind 1\\nwant 1\\nend 1\\nput 1\\...</td>\n","      <td>NaN</td>\n","      <td>said 7\\nput 2\\npushed 2\\nam 2\\nresponded 2\\nin...</td>\n","      <td>NaN</td>\n","      <td>said 4\\nhit 1\\npushing 1\\ntakes 1\\napproach 1\\...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>fended 1\\nwrote 1\\nhave 1</td>\n","      <td>introduced 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>impose 1</td>\n","      <td>NaN</td>\n","      <td>has 2\\ntells 2\\nsaid 1</td>\n","      <td>NaN</td>\n","      <td>said 5\\njoined 1\\nthink 1\\nasked 1\\nresponded ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>jul</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>abc-news</td>\n","      <td>Soo Rin Kim</td>\n","      <td>2020 candidates spend millions on online ads a...</td>\n","      <td>https://abcnews.go.com/Politics/love-hate-rela...</td>\n","      <td>2019-07-27T10:22:00Z</td>\n","      <td>\\nDemocrats vying for presidential election in...</td>\n","      <td>\\nDemocrats vying for presidential election in...</td>\n","      <td>burned 1</td>\n","      <td>NaN</td>\n","      <td>become 1\\nincluding 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>spent 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>filed 1\\nsuspending 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>spent 1</td>\n","      <td>NaN</td>\n","      <td>spent 1</td>\n","      <td>NaN</td>\n","      <td>said 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>spent 1\\nsaid 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>said 2\\nreleased 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>jul</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>abc-news</td>\n","      <td>Rick Klein and MaryAlice Parks</td>\n","      <td>The Note: Blue wave continues ride through Ken...</td>\n","      <td>https://abcnews.go.com/Politics/note-blue-wave...</td>\n","      <td>2019-11-06T11:05:10Z</td>\n","      <td>\\nThe TAKE with Rick Klein\\n \\nIt might just b...</td>\n","      <td>\\nThe TAKE with Rick Klein\\n \\nIt might just b...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>asked 2\\ndraw 1\\nboost 1\\npoised 1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>nov</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0    source  ... Bloomberg_adjectives month\n","0           0  abc-news  ...                  NaN   aug\n","1           1  abc-news  ...                  NaN   aug\n","2           2  abc-news  ...                  NaN   jul\n","3           3  abc-news  ...                  NaN   jul\n","4           4  abc-news  ...                  NaN   nov\n","\n","[5 rows x 53 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"8dpbye5QFDUN","colab_type":"code","colab":{}},"source":["# combine all text\n","a = data[\"coref_resolved\"]\n","all_data = \" \".join(data[\"coref_resolved\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JM-11Nm_6XZ9","colab_type":"text"},"source":["## Clean text"]},{"cell_type":"code","metadata":{"id":"teF9_KxE6mF-","colab_type":"code","colab":{}},"source":["punct_chars = list((set(string.punctuation) | {'»', '–', '—', '-',\"­\", '\\xad', '-', '◾', '®', '©','✓','▲', '◄','▼','►', '~', '|', '“', '”', '…', \"'\", \"`\", '_', '•', '*', '■'} - {\"'\"}))\n","punct_chars.sort()\n","punctuation = ''.join(punct_chars)\n","replace = re.compile('[%s]' % re.escape(punctuation))\n","sno = nltk.stem.SnowballStemmer('english')\n","printable = set(string.printable)\n","stopwords = set(stopwords.words('english'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xao5jW4E6agm","colab_type":"code","colab":{}},"source":["def clean_text(text, stem=False):\n","    # lower case\n","    text = text.lower()\n","    # eliminate urls\n","    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n","    # substitute all other punctuation with whitespace\n","    text = replace.sub(' ', text)\n","    # replace all whitespace with a single space\n","    text = re.sub(r'\\s+', ' ', text)\n","    # strip off spaces on either end\n","    text = text.strip()\n","    # make sure all chars are printable\n","    text = ''.join([c for c in text if c in printable])\n","    words = text.split()\n","    # remove numeric\n","    words = [w for w in words if not w.isdigit()]\n","    # stem non stopwords\n","    if stem:\n","      words = [sno.stem(w) if w not in stopwords else w for w in words]\n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjZuwyYPb8Gc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"302d065d-fbeb-44cc-af3f-ee351d9cf926","executionInfo":{"status":"ok","timestamp":1585370359458,"user_tz":420,"elapsed":920,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"FamPB-8K6Om-","colab_type":"code","colab":{}},"source":["# split text into sentences (nltk.sent_tokenize)\n","sentences = nltk.sent_tokenize(all_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfYNnSNL6w_r","colab_type":"code","colab":{}},"source":["# clean each sentence using the function clean_text()\n","cleaned_sentences = []\n","for sent in sentences:\n","  cleaned_sentences.append(clean_text(sent))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDk22GRJFFnN","colab_type":"text"},"source":["## Create model"]},{"cell_type":"code","metadata":{"id":"rA22PU7FFJTd","colab_type":"code","colab":{}},"source":["# Create vocabulary model\n","bigrams = phrases.Phrases(all_sentences, min_count=5, delimiter=b' ', common_terms=stopwords)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mp-9C0t8FzFh","colab_type":"code","colab":{}},"source":["# Create vocabulary of bigrams\n","print(\"Creating vocabulary...\")\n","data = bigrams[all_sentences]\n","vocab = [w for sent in data for w in sent]\n","vocab = [w for w, count in Counter(vocab).most_common() if count >= 5]\n","\n","# Save vocab\n","with codecs.open(os.path.join(root_dir + \"models\", 'word2vec_vocab.txt'), 'w', encoding='utf-8') as f:\n","    f.write('\\n'.join(vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cy-FLUdj8PXR","colab_type":"code","colab":{}},"source":["# Run word2vec model with bootstrapping\n","sample_size = int(0.8 * len(all_sentences))\n","NUM_RUNS = 20\n","for run_idx in range(NUM_RUNS):\n","    print(\"Run #%d\" % run_idx)\n","    data = bigrams[np.random.choice(all_sentences, sample_size, replace=True)]\n","    model = word2vec.Word2Vec(data, size=200, window=5, sg=1, min_count=5, workers=10)\n","    model.wv.save(os.path.join(root_dir + \"models\", \"word2vec_model_\" + str(run_idx) + '.wv'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ0MiA-FMs7R","colab_type":"code","colab":{}},"source":["# Run for each source\n","def run_word2vec_for_source(data, source):\n","  data = data[data[\"source\"] == source]\n","  all_text = \" \".join(data['coref_resolved'])\n","  sents = nltk.sent_tokenize(all_text)\n","  print(len(sents))\n","  all_sentences = [clean_text(sent) for sent in sents]\n","  bigrams = phrases.Phrases(all_sentences, min_count=5, delimiter=b' ', common_terms=stopwords)\n","  # Create vocabulary of bigrams\n","  print(\"Creating vocabulary...\")\n","  data = bigrams[all_sentences]\n","  vocab = [w for sent in data for w in sent]\n","  vocab = [w for w, count in Counter(vocab).most_common() if count >= 5]\n","\n","  # Save vocab\n","  with codecs.open(os.path.join(root_dir + \"models\", source + '_vocab.txt'), 'w', encoding='utf-8') as f:\n","      f.write('\\n'.join(vocab))\n","\n","  # Run word2vec model for source\n","  model = word2vec.Word2Vec(data, size=200, window=5, sg=1, min_count=5, workers=10)\n","  model.wv.save(os.path.join(root_dir + \"models\", source + \"_model.wv\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_C5paIcMwk4","colab_type":"code","colab":{}},"source":["sources = data.groupby(\"source\").filter(lambda x: len(x) > 200)[\"source\"].unique()\n","sources"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"de140bQmM4zW","colab_type":"text"},"source":["## Load models"]},{"cell_type":"code","metadata":{"id":"Hpsh8fzoc273","colab_type":"code","colab":{}},"source":["def get_models(filelist):\n","    model_files = [f for f in filelist if f.endswith('.wv')]\n","    models = [KeyedVectors.load(fname, mmap='r') for fname in model_files]\n","    return models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"20cuFG1IM3Gk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"482bffc6-e54f-4689-a86d-4ef7f6092d7b","executionInfo":{"status":"ok","timestamp":1585370609113,"user_tz":420,"elapsed":13931,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}}},"source":["filelist = []\n","for subdir, dirs, files in os.walk(root_dir + \"models/word2vec_bootstrap\"):\n","  for file in files:\n","    filelist.append(os.path.join(subdir, file))\n","models = get_models(filelist)\n","len(models)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"eYKV4FEdNAEo","colab_type":"code","colab":{}},"source":["# Get vocab (intersection of all vocab)\n","vocab = set(models[0].vocab)\n","for m in models:\n","    vocab &= set(m.vocab)\n","len(vocab)\n","idx2word = {i: w for i, w in enumerate(list(vocab))}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fofl1ed5BVKA","colab_type":"text"},"source":["# Look at word similarity"]},{"cell_type":"markdown","metadata":{"id":"0x2xt85kCnmC","colab_type":"text"},"source":["Check out this documentation: https://radimrehurek.com/gensim/models/word2vec.html"]},{"cell_type":"markdown","metadata":{"id":"p2NfCUPsBuKz","colab_type":"text"},"source":["### Find closest words to a query / set of queries"]},{"cell_type":"markdown","metadata":{"id":"rOMgY2nhDdYs","colab_type":"text"},"source":["Hint: use the `model.similarity()` function"]},{"cell_type":"code","metadata":{"id":"5wnyrKPVDvWU","colab_type":"code","colab":{}},"source":["def get_closest(queries, models, vocab, idx2word):\n","    cosines = []\n","    for m in models:\n","        cosines.append([np.mean([m.similarity(q, word) for q in queries]) for word in vocab])\n","    cosines = np.mean(np.array(cosines), axis=0)\n","    return [(idx2word[idx], cosines[idx]) for idx in cosines.argsort()[-20:][::-1]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I6SDBtngNH7_","colab_type":"code","colab":{}},"source":["def filter_words(words, vocab):\n","    words = set(words)\n","    not_in_vocab = words - vocab\n","    if not_in_vocab:\n","        print(\"Not in vocab:\")\n","        print(not_in_vocab)\n","    return list(words - not_in_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hk-JmVBBEPZb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"382691da-7b4b-45b4-bf67-aed36f4594ab","executionInfo":{"status":"ok","timestamp":1585370723800,"user_tz":420,"elapsed":42668,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}}},"source":["queries = [\"power\"] # this should be a list of queries (it can also be just one query)\n","\n"," # Remove queries not in vocab\n","queries = filter_words(queries, vocab)\n","\n","print(\"Getting most similar words...\")\n","closest = get_closest(queries, models, vocab, idx2word)\n","for (w, c) in closest:\n","    print(\"%s %.2f\" % (w, c))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Getting most similar words...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"stream","text":["power 1.00\n","personal gain 0.50\n","powers 0.49\n","abusing 0.48\n","authority 0.48\n","office to solicit 0.48\n","corrupting 0.47\n","trump abused 0.46\n","wield 0.45\n","levers 0.45\n","public trust 0.45\n","leverage 0.45\n","pressuring a foreign 0.45\n","exert 0.45\n","accused of abusing 0.45\n","personal interests 0.44\n","wields 0.44\n","speak truth 0.44\n","subvert 0.44\n","powers of the presidency 0.43\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qJR5kS8QCwAe","colab_type":"text"},"source":["#### Compare the similarity of a query to two words / two sets of words (e.g. candidate names)"]},{"cell_type":"markdown","metadata":{"id":"MHDo57DpGfVy","colab_type":"text"},"source":["For example, compare the similarity of the word \"power\" to two different candidate names (lowercase)."]},{"cell_type":"code","metadata":{"id":"VYWHuH-1C5jo","colab_type":"code","colab":{}},"source":["def get_cosines(name1, name2, words1, words2, queries, models):\n","    df_w1 = []\n","    df_w2 = []\n","    df_q = []\n","    df_type = []\n","    df_pvals = []\n","    for key, values in queries.items():\n","        for q in values:\n","            vals1 = [m.similarity(word1, q) for m in models for word1 in words1]\n","            vals2 = [m.similarity(word2, q) for m in models for word2 in words2]\n","            df_w1.append(np.mean(vals1))\n","            df_w2.append(np.mean(vals2))\n","            df_q.append(q)\n","            df_type.append(key)\n","            df_pvals.append(ttest_ind(vals1, vals2)[1])\n","    df = pd.DataFrame({name1: df_w1, name2: df_w2, 'query': df_q, 'word category': df_type, \"p value\": df_pvals})\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Q2GONwKDb2g","colab_type":"code","colab":{}},"source":["queries = {\"achievement\": [\"achieved\", \"success\", \"successful\"]}\n","\n","#{\"power\": [\"powerful\", \"power\", \"authority\", \"powers\", \"influence\"]}\n","words1 = [\"biden\", \"joe\", \"joe biden\"]\n","words2 =  [\"sanders\", \"bernie\", \"bernie sanders\"]\n","# Remove queries not in vocab\n","words1 = filter_words(words1, vocab)\n","words2 = filter_words(words2, vocab)\n","for k, v in queries.items():\n","    queries[k] = filter_words(v, vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FunK19H-Ne-L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":196},"outputId":"76a490fe-23a1-4347-fc02-f73ca2588796","executionInfo":{"status":"ok","timestamp":1585371466002,"user_tz":420,"elapsed":623,"user":{"displayName":"Audrey Acken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDPGVnp-A2z1GuB9-SIH_x7qs7_HLRi-3gp7FPw=s64","userId":"06876030775238787919"}}},"source":["get_cosines(\"biden\", \"sanders\", words1, words2, queries, models)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>biden</th>\n","      <th>sanders</th>\n","      <th>query</th>\n","      <th>word category</th>\n","      <th>p value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.168594</td>\n","      <td>0.172994</td>\n","      <td>success</td>\n","      <td>achievement</td>\n","      <td>0.716380</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.178649</td>\n","      <td>0.200800</td>\n","      <td>successful</td>\n","      <td>achievement</td>\n","      <td>0.048377</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.220130</td>\n","      <td>0.202273</td>\n","      <td>achieved</td>\n","      <td>achievement</td>\n","      <td>0.220743</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      biden   sanders       query word category   p value\n","0  0.168594  0.172994     success   achievement  0.716380\n","1  0.178649  0.200800  successful   achievement  0.048377\n","2  0.220130  0.202273    achieved   achievement  0.220743"]},"metadata":{"tags":[]},"execution_count":43}]}]}